{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f7c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone , ServerlessSpec\n",
    "\n",
    "# if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "#     os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n",
    "    \n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14501182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# embeding model retrun vector of 768 dimension\n",
    "# if you use openai model then change dimension to 1536\n",
    "# if you use google model then change dimension to 768\n",
    "# if you use other model then change dimension to your model vector dimension\n",
    "import time\n",
    "\n",
    "index_name1 = \"langchain-test-index1\"  # change if desired\n",
    "\n",
    "if not pc.has_index(index_name1):\n",
    " pc.create_index(\n",
    "    name=index_name1,\n",
    "    dimension=768, ## google model value is 768 is vector num open ai demnsison num differnt 1500 \n",
    "    metric=\"cosine\", ## angle rate kitni dem find karni hai words which are similar\n",
    "    # specify the number of replicas and the cloud region\n",
    "      # number of replicas for high availability\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index1 = pc.Index(index_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04528995230793953, -0.04655832797288895, -0.009813512675464153, -0.06546591222286224, 0.03772089630365372]\n"
     ]
    }
   ],
   "source": [
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(\n",
    "#     model=\"models/embedding-001\",\n",
    "#     google_api_key=\"AIzaSyB_xE8TBppZPrfNNAktxrzUiMBZzDjynrs\"  # ← yahan apni actual key daalna\n",
    "# )\n",
    "\n",
    "# # Test karo:\n",
    "# vector = embeddings.embed_query(\"hi pakistan!\")\n",
    "# print(vector[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f266382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: AIzaSyB_xE8TBppZPrfNNAktxrzUiMBZzDjynrs\n",
      "[0.02282451093196869, -0.03511931374669075, -0.05303274095058441, -0.07811421900987625, 0.04230687394738197]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load variables from .env\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(f\"API Key: {api_key}\")  # Debug print — should NOT be None\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API Key not loaded. Check your .env file.\")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=api_key\n",
    ")\n",
    "\n",
    "vector = embeddings.embed_query(\"who is qaid azam!\")\n",
    "print(vector[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cde041f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('PINECONE_API_KEY', 'pcsk_4nLckP_GSUXxrakUZo6PYt2mqb6RPanTEBQByzzjGj7gfeEcbtUkk8TtiBDpdDV8ALQNE2'), ('GOOGLE_API_KEY', 'AIzaSyB_xE8TBppZPrfNNAktxrzUiMBZzDjynrs')])\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "print(dotenv_values(\".env\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb7723f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: AIzaSyB_xE8TBppZPrfNNAktxrzUiMBZzDjynrs\n",
      "[0.05168594419956207, -0.030764883384108543, -0.03062233328819275, -0.02802734263241291, 0.01813093200325966]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Force load .env\n",
    "load_dotenv(dotenv_path=Path(\".env\"))\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(f\"API Key loaded: {api_key}\")  # ✅ For checking — remove later\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API Key not loaded. Check your .env file.\")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=api_key\n",
    ")\n",
    "\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "print(vector[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "367f8bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: AIzaSyB_xE8TBppZPrfNNAktxrzUiMBZzDjynrs\n",
      "[0.013553564436733723, -0.013713358901441097, -0.015986211597919464, -0.05194046348333359, 0.02254072204232216]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Manually read the .env file\n",
    "config = dotenv_values(\".env\")\n",
    "api_key = config.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "print(f\"API Key loaded: {api_key}\")  # Just to verify\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API Key not found in .env file.\")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=api_key\n",
    ")\n",
    "\n",
    "vector = embeddings.embed_query(\"hi !\")\n",
    "print(vector[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whats semntaic search \n",
    "# semantic search is a technique used to improve search accuracy by understanding the context and meaning of the query rather than just matching keywords. It uses natural language processing (NLP) and machine learning to interpret user intent and retrieve relevant results based on the semantics of the query.\n",
    "# This approach allows for more nuanced and relevant search results, as it considers the relationships between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3272d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma also database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(\n",
    "    index =index1,\n",
    "    embedding=embeddings,\n",
    "    namespace=\"langchain-test-namespace-ahmad\"  # Optional, can be used to segment data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19409088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test1'}, page_content='Hello, world! This a is a test document.'),\n",
       " Document(metadata={}, page_content='Pinecone is a vector b  database for machine learning applications.'),\n",
       " Document(metadata={}, page_content='LangChain is a framework  c for building applications with LLMs.'),\n",
       " Document(metadata={}, page_content='Semantic search improves d search accuracy by understanding context and meaning.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "docs = [\n",
    "    Document(page_content=\"Hello, world! This a is a test document.\" , metadata={\"source\": \"test1\"}),\n",
    "    Document(page_content=\"Pinecone is a vector b  database for machine learning applications.\"),\n",
    "    Document(page_content=\"LangChain is a framework  c for building applications with LLMs.\"),\n",
    "    Document(page_content=\"Semantic search improves d search accuracy by understanding context and meaning.\"),\n",
    "   \n",
    "    \n",
    "]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93743692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data save \n",
    "\n",
    "# data retrive\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content= \"hi hows 2 u\",\n",
    "    metadata = {\"source\":\"tweet\"}\n",
    ")\n",
    "\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content= \"hi hows3 bye\",\n",
    "    metadata = {\"source\":\"fb\"}\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content= \"hi hows 4 ahamd11\",\n",
    "    metadata = {\"source\":\"837\"}\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1 ,\n",
    "    document_2 , \n",
    "    document_3 , \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca73ee9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b08d7029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('16a285b1-beff-498f-8e33-68b04bf38a45')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdd381f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a41bb4a9-cd42-43e0-85a0-0ced4e6f30da',\n",
       " '09f46587-cdaf-409f-9cce-d440fae1a5ee',\n",
       " 'fba05a00-2ff0-4ded-b257-78f8b4a1fdac']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "# create\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcc657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "batch_size = 50\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch_docs = documents[i:i+batch_size]\n",
    "    batch_uuids = [str(uuid4()) for _ in range(len(batch_docs))]\n",
    "    vector_store.add_documents(documents=batch_docs, ids=batch_uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "790a66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive\n",
    "results = vector_store.similarity_search(\n",
    "    query=\"hi hows u\", \n",
    "    k=3,  # Number of results to return\n",
    "    filter=None  # Optional filter to apply\n",
    "    \n",
    "    )\n",
    "for result in results:\n",
    "    print(f\"Content: {result.page_content}, Metadata: {result.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "504c21c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: hi hows 2 u, Metadata: {'source': 'tweet'}, Score: 0.756083\n",
      "Content: hi hows 2 u, Metadata: {'source': 'tweet'}, Score: 0.756083\n",
      "Content: hi hows3 bye, Metadata: {'source': 'fb'}, Score: 0.719344\n"
     ]
    }
   ],
   "source": [
    "# retrive\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    query=\"hi hows u\", \n",
    "    k=3,  # Number of results to return\n",
    "    filter=None  # Optional filter to apply\n",
    "    \n",
    "    )\n",
    "for result,score in results:\n",
    "    print(f\"Content: {result.page_content}, Metadata: {result.metadata}, Score: {score:3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a1c30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07209e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str):\n",
    "#   vector search\n",
    " Vector_results = vector_store.similarity_search(query , k=2) \n",
    "\n",
    "#  pass to model vector results + user query\n",
    "\n",
    " final_answer = llm.invoke(f\"ANSWER THIS IS USER QUERY: {query} \\n\\n here is some answer {Vector_results}\"  )\n",
    "#  ChatGoogleGenAI(results , query)\n",
    " return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9551fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jawab_agya =answer_query(\"hi how!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d501e352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The user said \"hi how!\".  The provided documents both contain \"hi hows3 bye\".  While the documents are similar to the user\\'s greeting, they aren\\'t an exact match.  A suitable response might be:\\n\\n\"Hi there!  I see you said \\'hi how!\\'  Is there anything specific you\\'d like to know or do?\"'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jawab_agya.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8f4a33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: hi hows 2 u, Metadata: {'source': 'tweet'}, Score: 0.756083\n",
      "Content: hi hows 2 u, Metadata: {'source': 'tweet'}, Score: 0.756083\n",
      "Content: hi hows3 bye, Metadata: {'source': 'fb'}, Score: 0.719344\n"
     ]
    }
   ],
   "source": [
    "def answer_query(query):\n",
    "    results = vector_store.similarity_search_with_score(\n",
    "        query=query, \n",
    "        k=3,  # Number of results to return\n",
    "        filter=None  # Optional filter to apply\n",
    "    )\n",
    "for result, score in results:\n",
    "    print(f\"Content: {result.page_content}, Metadata: {result.metadata}, Score: {score:3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8190f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
